{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import torchvision\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Online_Detectors import OnlineShiftDetectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "Here, we will import the CIFAR-10 dataset and the Alt-CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_data():\n",
    "    \n",
    "    n_epochs   = 150\n",
    "    batch_size = int(1e2)\n",
    "    lr         = 0.01\n",
    "\n",
    "    # define series of transforms to pre process images \n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "    # load training set \n",
    "    cifar10_trainset = torchvision.datasets.CIFAR10('/home/fmejia/fmejia/Cypercat/cyphercat/datasets//', train=True, transform=transform, download=True)\n",
    "    cifar10_trainloader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "\n",
    "    # load test set \n",
    "    cifar10_testset = torchvision.datasets.CIFAR10('/home/fmejia/fmejia/Cypercat/cyphercat//datasets//', train=False, transform=transform, download=True)\n",
    "    cifar10_testloader = torch.utils.data.DataLoader(cifar10_testset, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "\n",
    "    train_transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(256),\n",
    "        torchvision.transforms.Resize(32),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    gen_testloader = torch.load('/home/jgornet/Generative_Models/Covariate_Measurement_Models/altcifar_dataloader.pth')\n",
    "\n",
    "    return cifar10_trainloader,cifar10_testloader,gen_testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_trainloader,cifar10_testloader,gen_testloader = set_up_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up classifier\n",
    "This is the implementation for using the classifier label and covariate shift detector. For now, we will just use a pretrained model that was trained on Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model    = torchvision.models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(512,10)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_detector = OnlineShiftDetectors.ClassifierDetector(0.01,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to using the shift detector for classifiers, you can either compare the model's output with the true label distribution (detecting label shift), or compare one model's output for one dataset compared to another dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_detector.set_label_pred_distribution(model,cifar10_trainloader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.zeros([classifier_detector.class_size,len(cifar10_trainloader)])\n",
    "g = torch.zeros([len(cifar10_trainloader)])\n",
    "\n",
    "for i, batch in enumerate(cifar10_trainloader, 0):\n",
    "    \n",
    "    imgs, labels = batch\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "    \n",
    "    model_output = model(imgs.to(device))\n",
    "    r[:,i],g[i]  = classifier_detector.shift_filter(model_output.cpu(),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_detector.shift_filter(model_output.cpu(),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Covariate Shift\n",
    "This is the implementation for detecting covariate shift using an variational autoencoder. Here we train the model with Pytorch Lightning, which gives fast deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "import functools\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class ResnetGenerator(LightningModule):\n",
    "    \"\"\"Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n",
    "    We adapt Torch code and idea from Justin Johnson's neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,learning_rate=0.001):\n",
    "        \"\"\"Construct a Resnet-based generator\n",
    "        Parameters:\n",
    "            input_nc (int)      -- the number of channels in input images\n",
    "            output_nc (int)     -- the number of channels in output images\n",
    "            ngf (int)           -- the number of filters in the last conv layer\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers\n",
    "            n_blocks (int)      -- the number of ResNet blocks\n",
    "            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n",
    "        \"\"\"\n",
    "        use_dropout = True\n",
    "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
    "        # norm_layer = functools.partial(nn.InstanceNorm2d)\n",
    "        input_nc = 3\n",
    "        output_nc = 3\n",
    "        ngf = 64\n",
    "        ndf = 64\n",
    "\n",
    "        z_dim = 256\n",
    "\n",
    "        n_blocks=6\n",
    "        padding_type='reflect'\n",
    "        \n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.LeakyReLU(0.2, inplace = True)]\n",
    "\n",
    "        n_downsampling = 5\n",
    "        for i in range(n_downsampling):  # add downsampling layers\n",
    "            mult = 2 ** i\n",
    "            if i < (n_downsampling-1):\n",
    "                model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
    "                          norm_layer(ngf * mult * 2),\n",
    "                          nn.LeakyReLU(0.2, inplace = True)]\n",
    "            else:\n",
    "                model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias)]\n",
    "                     \n",
    "                    \n",
    "        ## variance model\n",
    "        model2 = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.LeakyReLU(0.2, inplace = True)]\n",
    "\n",
    "        n_downsampling = 5\n",
    "        for i in range(n_downsampling):  # add downsampling layers\n",
    "            mult = 2 ** i\n",
    "            if i < (n_downsampling-1):\n",
    "                model2 += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
    "                          norm_layer(ngf * mult * 2),\n",
    "                          nn.LeakyReLU(0.2, inplace = True)]\n",
    "            else:\n",
    "                model2 += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias)]\n",
    "                     \n",
    "               \n",
    "        model_upsample1 = []\n",
    "        \n",
    "        for i in range(n_downsampling-2):\n",
    "            mult = 2 ** (n_downsampling-i)\n",
    "            model_upsample1 += [                      \n",
    "#                     nn.Conv2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, padding =1, stride = 1),            \n",
    "#                     nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "                \n",
    "                      nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=use_bias),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.LeakyReLU(0.2, inplace = True)]\n",
    "        n_downsampling = 2\n",
    "        mult = 2 ** (n_downsampling)        \n",
    "        model_resnet = []\n",
    "        for i in range(n_blocks):       # add ResNet blocks\n",
    "\n",
    "            model_resnet += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
    "        \n",
    "        model_upsample = []\n",
    "        for i in range(n_downsampling):  # add upsampling layers\n",
    "            mult = 2 ** (n_downsampling - i)\n",
    "            model_upsample += [\n",
    "#                       nn.Conv2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, padding =1, stride = 1),            \n",
    "#                       nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "                    nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=use_bias),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.LeakyReLU(0.2, inplace = True)]\n",
    "        model_upsample += [nn.ReflectionPad2d(3)]\n",
    "        model_upsample += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "        model_upsample += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "        self.model_variance = nn.Sequential(*model2)\n",
    "        self.model_resnet = nn.Sequential(*model_resnet)\n",
    "        self.model_upsample1 = nn.Sequential(*model_upsample1)    \n",
    "        self.model_upsample = nn.Sequential(*model_upsample)\n",
    "        \n",
    "        self.mean = torch.tensor((0.4914, 0.4822, 0.4465))#.to(device)\n",
    "        self.mean = self.mean.view(-1,1,1)\n",
    "        self.var = torch.tensor((0.2023, 0.1994, 0.2010))#.to(device)\n",
    "        self.var = self.var.view(-1,1,1)\n",
    "\n",
    "        self.loss_function = nn.SmoothL1Loss()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, input, decode = False):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        if decode:                 \n",
    "                x = self.model_upsample1(input)            \n",
    "                x = self.model_resnet(x)\n",
    "                x = self.model_upsample(x)\n",
    "                x = x / 2 + 0.5\n",
    "                x = (x - self.mean)/self.var\n",
    "                return x\n",
    "        mean = self.model(input)\n",
    "        variance = self.model_variance(input)\n",
    "\n",
    "        sample = Variable(torch.randn(mean.size()).type(torch.cuda.FloatTensor))\n",
    "        x1 = mean + (variance * sample)\n",
    "        x = self.model_upsample1(x1)\n",
    "        x = self.model_resnet(x)\n",
    "        \n",
    "        x = self.model_upsample(x)\n",
    "        x = x / 2 + 0.5\n",
    "        x = (x - self.mean.cuda())/self.var.cuda()\n",
    "        return x, x1, mean, variance\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        imgs, labels = batch\n",
    "        \n",
    "        out_img, embed_out, mean, variance = self(imgs)\n",
    "        \n",
    "        AE_loss = self.loss_function(out_img, imgs)\n",
    "        kl_loss = (mean ** 2 + variance **2 - torch.log(variance ** 2) - 1).mean()\n",
    "        loss    = AE_loss + kl_loss/10\n",
    "        \n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss         = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        imgs, labels = batch\n",
    "        \n",
    "        out_img, embed_out, mean, variance = self(imgs)\n",
    "        AE_loss = self.loss_function(out_img, imgs)\n",
    "        kl_loss = (mean ** 2 + variance **2 - torch.log(variance ** 2) - 1).mean()\n",
    "        loss    = AE_loss + kl_loss/10\n",
    "        \n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        \n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        beta1   = 0.5\n",
    "        lr_adam = 1e-04\n",
    "        #optimizer_g = torch.optim.Adam(Generator.parameters(), lr = lr_adam, betas = (beta1, 0.999))\n",
    "\n",
    "        return torch.optim.Adam(self.parameters(), lr=(self.learning_rate), betas = (beta1, 0.999))\n",
    "    \n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"Define a Resnet block\"\"\"\n",
    "\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        \"\"\"Initialize the Resnet block\n",
    "        A resnet block is a conv block with skip connections\n",
    "        We construct a conv block with build_conv_block function,\n",
    "        and implement skip connections in <forward> function.\n",
    "        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n",
    "        \"\"\"\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "\n",
    "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        \"\"\"Construct a convolutional block.\n",
    "        Parameters:\n",
    "            dim (int)           -- the number of channels in the conv layer.\n",
    "            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "            use_bias (bool)     -- if the conv layer uses bias or not\n",
    "        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n",
    "        \"\"\"\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.LeakyReLU(0.2, inplace = True)]\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.2)]\n",
    "\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function (with skip connections)\"\"\"\n",
    "        out = x + self.conv_block(x)  # add skip connections\n",
    "        return out\n",
    "    \n",
    "use_dropout = True\n",
    "norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
    "# norm_layer = functools.partial(nn.InstanceNorm2d)\n",
    "input_nc = 3\n",
    "output_nc = 3\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "\n",
    "z_dim = 256\n",
    "\n",
    "model = ResnetGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "trainer = Trainer(gpus=4, num_nodes=1, distributed_backend='dp',auto_lr_find=True,profiler=True,max_epochs=3,default_root_dir='/home/jgornet/Generative_Models/VAE_Covariate_Detect/')\n",
    "trainer.fit(model, cifar10_trainloader, cifar10_testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"/home/jgornet/Generative_Models/VAE_Covariate_Detect/VAE_Covariate_Detect.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResnetGenerator().load_from_checkpoint(checkpoint_path=\"/home/jgornet/Generative_Models/VAE_Covariate_Detect/VAE_Covariate_Detect.ckpt\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_detector = OnlineShiftDetectors.VariationalDetector(0.1,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_variable_list = variational_detector.set_latent_distribution(model,cifar10_trainloader,device,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Tests\n",
    "\n",
    "These tests are made to show the detection signal for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train = np.array([])\n",
    "\n",
    "for epoch in range(1):\n",
    "    \n",
    "    for i, batch in enumerate(cifar10_trainloader, 0):\n",
    "\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        _, embed_out, _, _   = model(imgs.to(device))\n",
    "        r_var                = variational_detector.shift_filter(embed_out.cpu(),100)\n",
    "        r_train              = np.append(r_train,r_var.cpu().detach().numpy())    \n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(r_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_test = np.array([])\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    for i, batch in enumerate(cifar10_testloader, 0):\n",
    "\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        _, embed_out, _, _   = model(imgs.to(device))\n",
    "        r_var                = variational_detector.shift_filter(embed_out.cpu(),100)\n",
    "        r_test               = np.append(r_test,r_var.cpu().detach().numpy())    \n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(r_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_gen = np.array([])\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    for i, batch in enumerate(gen_testloader, 0):\n",
    "\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        _, embed_out, _, _   = model(imgs.to(device))\n",
    "        r_var                = variational_detector.shift_filter(embed_out.cpu(),100)\n",
    "        r_gen                = np.append(r_gen,r_var.cpu().detach().numpy())    \n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(r_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r_train)\n",
    "plt.plot(r_test)\n",
    "plt.plot(r_gen)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,0.1,100)\n",
    "prob_train,_ = np.histogram(r_train,bins)\n",
    "prob_test,_  = np.histogram(r_test,bins)\n",
    "prob_gen,_   = np.histogram(r_gen,bins)\n",
    "\n",
    "prob_train = prob_train/prob_train.sum()\n",
    "prob_test  = prob_test/prob_test.sum()\n",
    "prob_gen   = prob_gen/prob_gen.sum()\n",
    "\n",
    "plt.plot(bins[:99],prob_train)\n",
    "plt.plot(bins[:99],prob_test)\n",
    "plt.plot(bins[:99],prob_gen)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
